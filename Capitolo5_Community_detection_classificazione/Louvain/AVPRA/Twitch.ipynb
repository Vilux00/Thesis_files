{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "copyrighted-raleigh",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Import useful libraries\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "import networkx as nx\n",
    "import time\n",
    "import random\n",
    "import csv\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "first-membership",
   "metadata": {},
   "source": [
    "### Only lang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "usual-cooperation",
   "metadata": {},
   "outputs": [],
   "source": [
    "l = list(range(0, 10)) + list(range(10, 30, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "printable-westminster",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Reading VLs from file\n",
    "obj = pd.read_pickle(\"./Only_lang/log_trial_0_LPStates.pickled\")\\\n",
    "    + pd.read_pickle(\"./Only_lang/log_trial_1_LPStates.pickled\")[1:]\\\n",
    "        +  pd.read_pickle(\"./Only_lang/log_trial_2_LPStates.pickled\")[1:]\\\n",
    "            + pd.read_pickle(\"./Only_lang/log_trial_3_LPStates.pickled\")[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "acting-joseph",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"./twitch_comms.csv\", sep=\" \", header=None)\n",
    "comms_dict = {}\n",
    "for row in data.iterrows():\n",
    "    comms_dict[str(row[1][0])] = row[1][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "democratic-island",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration completed in 3.797091007232666\n",
      "Iteration completed in 12.508708000183105\n",
      "Iteration completed in 51.834871768951416\n",
      "Iteration completed in 52.75838112831116\n",
      "Iteration completed in 53.3145968914032\n",
      "Iteration completed in 53.30100679397583\n",
      "Iteration completed in 53.077017307281494\n",
      "Iteration completed in 54.61321949958801\n",
      "Iteration completed in 49.390111684799194\n",
      "Iteration completed in 50.49282169342041\n",
      "Iteration completed in 43.771607875823975\n",
      "Iteration completed in 48.585078954696655\n",
      "Iteration completed in 47.025930643081665\n",
      "Iteration completed in 45.32200050354004\n",
      "Iteration completed in 50.47292685508728\n",
      "Iteration completed in 49.30631709098816\n",
      "Iteration completed in 48.08205485343933\n",
      "Iteration completed in 50.38036823272705\n",
      "Iteration completed in 52.70094299316406\n",
      "Iteration completed in 53.58765196800232\n",
      "Iteration completed in 52.1880145072937\n",
      "Iteration completed in 48.52446794509888\n",
      "Iteration completed in 45.81927704811096\n",
      "Iteration completed in 46.41480350494385\n",
      "Iteration completed in 46.61470675468445\n",
      "Iteration completed in 47.92875647544861\n",
      "Iteration completed in 47.3267605304718\n",
      "Iteration completed in 48.82628583908081\n",
      "Iteration completed in 53.734347105026245\n"
     ]
    }
   ],
   "source": [
    "### Random forest classifier creation with 70 trees\n",
    "clf = RandomForestClassifier(n_estimators=70)\n",
    "start_time = time.time()\n",
    "accuracies = []\n",
    "f1_scores_macro = []\n",
    "f1_scores_weigh = []\n",
    "for res in obj:\n",
    "    s_time = time.time()\n",
    "    # Input \n",
    "    X_data = res[1]\n",
    "    # Output communities defined by Louvain algorithm\n",
    "    y_data = [comms_dict.get(str(i)) for i in range(1, len(comms_dict) + 1)]\n",
    "\n",
    "    # Split the data into training set and test set\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_data, y_data, test_size=0.2)\n",
    "\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = clf.predict(X_test)\n",
    "\n",
    "    ### Accuracy metric\n",
    "    accuracies.append(metrics.accuracy_score(y_test, y_pred))\n",
    "    f1_scores_macro.append(metrics.f1_score(y_test, y_pred, average=\"macro\"))\n",
    "    f1_scores_weigh.append(metrics.f1_score(y_test, y_pred, average=\"weighted\"))\n",
    "\n",
    "    print(f\"Iteration completed in {time.time() - s_time}\")\n",
    "end_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "noticed-wagner",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Function that returns the 10 / 1 index of the maximum values of a list\n",
    "def get10maxidx(l):\n",
    "    return list(map(lambda x: x[1], sorted(zip(l, range(0, len(l))), reverse=True)[:10]))\n",
    "def getmaxidx(l):\n",
    "    return l.index(max(l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "specialized-latter",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 10MWL classification\n",
    "clf = RandomForestClassifier(n_estimators=70)\n",
    "start_time2 = time.time()\n",
    "accuracies2 = []\n",
    "f1_scores2_macro = []\n",
    "f1_scores2_weigh = []\n",
    "for res in obj:\n",
    "    # Input \n",
    "    X_data = list(map(lambda x: get10maxidx(x), res[1]))\n",
    "\n",
    "    # Output communities defined by Louvain algorithm\n",
    "    y_data = [comms_dict.get(str(i)) for i in range(1, len(comms_dict) + 1)]\n",
    "\n",
    "    # Split the data into training set and test set\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_data, y_data, test_size=0.2)\n",
    "\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = clf.predict(X_test)\n",
    "\n",
    "    ### Accuracy metric\n",
    "    accuracies2.append(metrics.accuracy_score(y_test, y_pred))\n",
    "    f1_scores2_macro.append(metrics.f1_score(y_test, y_pred, average=\"macro\"))\n",
    "    f1_scores2_weigh.append(metrics.f1_score(y_test, y_pred, average=\"weighted\"))\n",
    "    \n",
    "end_time2 = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "empirical-background",
   "metadata": {},
   "outputs": [],
   "source": [
    "### MWL classification\n",
    "clf = RandomForestClassifier(n_estimators=70)\n",
    "start_time3 = time.time()\n",
    "accuracies3 = []\n",
    "f1_scores3_macro = []\n",
    "f1_scores3_weigh = []\n",
    "for res in obj:\n",
    "    # Input \n",
    "    X_data = list(map(lambda x: [getmaxidx(x)], res[1]))\n",
    "\n",
    "    # Output communities defined by Louvain algorithm\n",
    "    y_data = [comms_dict.get(str(i)) for i in range(1, len(comms_dict) + 1)]\n",
    "\n",
    "    # Split the data into training set and test set\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_data, y_data, test_size=0.2)\n",
    "\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = clf.predict(X_test)\n",
    "\n",
    "    ### Accuracy metric\n",
    "    accuracies3.append(metrics.accuracy_score(y_test, y_pred))\n",
    "    f1_scores3_macro.append(metrics.f1_score(y_test, y_pred, average=\"macro\"))\n",
    "    f1_scores3_weigh.append(metrics.f1_score(y_test, y_pred, average=\"weighted\"))\n",
    "    \n",
    "end_time3 = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "welsh-growth",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "matplotlib.use(\"pgf\")\n",
    "matplotlib.rcParams.update({\n",
    "    \"pgf.texsystem\": \"pdflatex\",\n",
    "    'font.family': 'serif',\n",
    "    'text.usetex': True,\n",
    "    'pgf.rcfonts': False\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wrapped-omaha",
   "metadata": {},
   "source": [
    "### Comparison macro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "functional-latin",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_16030/1686280980.py:18: UserWarning: Matplotlib is currently using pgf, which is a non-GUI backend, so cannot show the figure.\n",
      "  plt.show()\n"
     ]
    }
   ],
   "source": [
    "# Plot F1-macro comparison\n",
    "plt.figure(figsize=(10, 6))\n",
    "l = list(range(0,10)) + list(range(10, 30, 2))\n",
    "plt.plot(l, [f1_scores_macro[i] for i in l], \"o\", label=\"AVPRA F1-score-macro\", markersize=10)\n",
    "plt.plot(l, [f1_scores2_macro[i] for i in l], \"o\", label=\"AVPRA 10MWL F1-score-macro\", markersize=10)\n",
    "plt.plot(l, [f1_scores3_macro[i] for i in l], \"o\", label=\"AVPRA MWL F1-score-macro\", markersize=10)\n",
    "\n",
    "plt.axvline(x=7, label=\"Diametro\", linestyle=\"--\")\n",
    "\n",
    "plt.xlabel(\"Iterazione\", fontsize=20)\n",
    "plt.ylabel(\"F1-score\", fontsize=20)\n",
    "plt.legend(loc=\"lower right\", prop={'size': 16})\n",
    "plt.xticks(fontsize=16)\n",
    "plt.yticks(fontsize=16)\n",
    "plt.ylim(0,1)\n",
    "\n",
    "plt.savefig(\"F1_twitch_onlyL_AVPRA_all_macro.png\", dpi=500)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "distinct-memorial",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8739236820477795, 10)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(f1_scores_macro), f1_scores_macro.index(max(f1_scores_macro))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "auburn-agreement",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.7219649344059615, 5)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(f1_scores2_macro), f1_scores2_macro.index(max(f1_scores2_macro))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "urban-belle",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.6342339832223101, 1)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(f1_scores3_macro), f1_scores3_macro.index(max(f1_scores3_macro))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "requested-color",
   "metadata": {},
   "source": [
    "### Comparison weighted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "legal-encounter",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_16030/1241856322.py:18: UserWarning: Matplotlib is currently using pgf, which is a non-GUI backend, so cannot show the figure.\n",
      "  plt.show()\n"
     ]
    }
   ],
   "source": [
    "# Plot F1-macro comparison\n",
    "plt.figure(figsize=(10, 6))\n",
    "l = list(range(0,10)) + list(range(10, 30, 2))\n",
    "plt.plot(l, [f1_scores_weigh[i] for i in l], \"o\", label=\"AVPRA F1-score-weighted\", markersize=10)\n",
    "plt.plot(l, [f1_scores2_weigh[i] for i in l], \"o\", label=\"AVPRA 10MWL F1-score-weighted\", markersize=10)\n",
    "plt.plot(l, [f1_scores3_weigh[i] for i in l], \"o\", label=\"AVPRA MWL F1-score-weighted\", markersize=10)\n",
    "\n",
    "plt.axvline(x=7, label=\"Diametro\", linestyle=\"--\")\n",
    "\n",
    "plt.xlabel(\"Iterazione\", fontsize=20)\n",
    "plt.ylabel(\"F1-score\", fontsize=20)\n",
    "plt.legend(loc=\"lower right\", prop={'size': 16})\n",
    "plt.xticks(fontsize=16)\n",
    "plt.yticks(fontsize=16)\n",
    "plt.ylim(0,1)\n",
    "\n",
    "plt.savefig(\"F1_twitch_onlyL_AVPRA_all_weighted.png\", dpi=500)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "limited-video",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8338172346497804, 11)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(f1_scores_weigh), f1_scores_weigh.index(max(f1_scores_weigh))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "tired-arbitration",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.633068269396643, 5)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(f1_scores2_weigh), f1_scores2_weigh.index(max(f1_scores2_weigh))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "biological-stopping",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.3613434339331933, 1)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(f1_scores3_weigh), f1_scores3_weigh.index(max(f1_scores3_weigh))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "under-vintage",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_16030/4171820616.py:17: UserWarning: Matplotlib is currently using pgf, which is a non-GUI backend, so cannot show the figure.\n",
      "  plt.show()\n"
     ]
    }
   ],
   "source": [
    "# Plot accuracy graph\n",
    "plt.figure(figsize=(10, 6))\n",
    "l = list(range(0,10)) + list(range(10, 30, 2))\n",
    "plt.plot(l, [accuracies[i] for i in l], \"o\", label=\"AVPRA Accuratezza\", markersize=10)\n",
    "plt.plot(l, [f1_scores_macro[i] for i in l], \"x\", label=\"AVPRA F1-score-macro\", color=\"blue\", markersize=12)\n",
    "\n",
    "plt.axvline(x=7, label=\"Diametro\", linestyle=\"--\")\n",
    "\n",
    "plt.xlabel(\"Iterazione\", fontsize=20)\n",
    "plt.ylabel(\"Accuratezza/F1-Score\", fontsize=20)\n",
    "plt.legend(loc=\"right\", prop={'size': 16})\n",
    "plt.xticks(fontsize=16)\n",
    "plt.yticks(fontsize=16)\n",
    "plt.ylim(0,1)\n",
    "\n",
    "plt.savefig(\"F1_twitch_onlyL_AVPRA_macro.png\", dpi=500)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "express-consensus",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_16030/646034198.py:17: UserWarning: Matplotlib is currently using pgf, which is a non-GUI backend, so cannot show the figure.\n",
      "  plt.show()\n"
     ]
    }
   ],
   "source": [
    "# Plot accuracy graph\n",
    "plt.figure(figsize=(10, 6))\n",
    "l = list(range(0,10)) + list(range(10, 30, 2))\n",
    "plt.plot(l, [accuracies[i] for i in l], \"o\", label=\"AVPRA Accuratezza\", markersize=10)\n",
    "plt.plot(l, [f1_scores_weigh[i] for i in l], \"x\", label=\"AVPRA F1-score-weighted\", color=\"blue\", markersize=12)\n",
    "\n",
    "plt.axvline(x=7, label=\"Diametro\", linestyle=\"--\")\n",
    "\n",
    "plt.xlabel(\"Iterazione\", fontsize=20)\n",
    "plt.ylabel(\"Accuratezza/F1-Score\", fontsize=20)\n",
    "plt.legend(loc=\"right\", prop={'size': 16})\n",
    "plt.xticks(fontsize=16)\n",
    "plt.yticks(fontsize=16)\n",
    "plt.ylim(0,1)\n",
    "\n",
    "plt.savefig(\"F1_twitch_onlyL_AVPRA_weighted.png\", dpi=500)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "abandoned-computer",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8394848764238765, 12)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(accuracies), l[accuracies.index(max(accuracies))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dominican-trance",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.6444695595277042, 8)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(accuracies2), l[accuracies2.index(max(accuracies2))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "norwegian-alberta",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.4918954287243851, 0)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(accuracies3), l[accuracies3.index(max(accuracies3))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "authorized-seller",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_16030/3905670998.py:18: UserWarning: Matplotlib is currently using pgf, which is a non-GUI backend, so cannot show the figure.\n",
      "  plt.show()\n"
     ]
    }
   ],
   "source": [
    "# Plot accuracy graph\n",
    "plt.figure(figsize=(10, 6))\n",
    "l = list(range(0,10)) + list(range(10, 30, 2))\n",
    "plt.plot(l, [accuracies[i] for i in l], \"o\", label=\"AVPRA Accuratezza\", markersize=10)\n",
    "plt.plot(l, [accuracies2[i] for i in l], \"o\", label=\"AVPRA 10MWL Accuratezza\", markersize=10)\n",
    "plt.plot(l, [accuracies3[i] for i in l], \"o\", label=\"AVPRA MWL Accuratezza\", markersize=10)\n",
    "\n",
    "plt.axvline(x=7, label=\"Diametro\", linestyle=\"--\")\n",
    "\n",
    "plt.xlabel(\"Iterazione\", fontsize=20)\n",
    "plt.ylabel(\"Accuratezza\", fontsize=20)\n",
    "plt.legend(loc=\"lower right\", prop={'size': 16})\n",
    "plt.xticks(fontsize=16)\n",
    "plt.yticks(fontsize=16)\n",
    "plt.ylim(0,1)\n",
    "\n",
    "plt.savefig(\"F1_comparisons_twitch_onlyF.png\", dpi=500)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "still-rendering",
   "metadata": {},
   "source": [
    "# All features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "choice-portal",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Reading VLs from file\n",
    "obj = pd.read_pickle(\"./All_feat/log_trial_0_LPStates.pickled\")\\\n",
    "    + pd.read_pickle(\"./All_feat/log_trial_1_LPStates.pickled\")[1:]\\\n",
    "        +  pd.read_pickle(\"./All_feat/log_trial_2_LPStates.pickled\")[1:]\\\n",
    "            + pd.read_pickle(\"./All_feat/log_trial_3_LPStates.pickled\")[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "meaningful-monitoring",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration completed in 4.777518272399902\n",
      "Iteration completed in 17.610280990600586\n",
      "Iteration completed in 57.12461590766907\n",
      "Iteration completed in 69.41503095626831\n",
      "Iteration completed in 57.900755643844604\n",
      "Iteration completed in 68.97751665115356\n",
      "Iteration completed in 61.19676685333252\n",
      "Iteration completed in 61.75340533256531\n",
      "Iteration completed in 54.12453317642212\n",
      "Iteration completed in 60.472928285598755\n",
      "Iteration completed in 57.43724083900452\n",
      "Iteration completed in 60.124637842178345\n",
      "Iteration completed in 53.83508777618408\n",
      "Iteration completed in 62.17167949676514\n",
      "Iteration completed in 57.288023710250854\n",
      "Iteration completed in 56.26481294631958\n",
      "Iteration completed in 56.660359621047974\n",
      "Iteration completed in 72.802161693573\n",
      "Iteration completed in 127.16159868240356\n",
      "Iteration completed in 135.9566969871521\n"
     ]
    }
   ],
   "source": [
    "### Random forest classifier creation with 70 trees\n",
    "clf = RandomForestClassifier(n_estimators=70)\n",
    "start_time = time.time()\n",
    "accuracies_allf = []\n",
    "f1_scores_macro_allf = []\n",
    "f1_scores_weigh_allf = []\n",
    "for res in [obj[i] for i in l]:\n",
    "    s_time = time.time()\n",
    "    # Input \n",
    "    X_data = res[1]\n",
    "    # Output communities defined by Louvain algorithm\n",
    "    y_data = [comms_dict.get(str(i)) for i in range(1, len(comms_dict) + 1)]\n",
    "\n",
    "    # Split the data into training set and test set\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_data, y_data, test_size=0.2)\n",
    "\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = clf.predict(X_test)\n",
    "\n",
    "    ### Accuracy metric\n",
    "    accuracies_allf.append(metrics.accuracy_score(y_test, y_pred))\n",
    "    f1_scores_macro_allf.append(metrics.f1_score(y_test, y_pred, average=\"macro\"))\n",
    "    f1_scores_weigh_allf.append(metrics.f1_score(y_test, y_pred, average=\"weighted\"))\n",
    "\n",
    "    print(f\"Iteration completed in {time.time() - s_time}\")\n",
    "end_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "expensive-quantum",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Function that returns the 10 / 1 index of the maximum values of a list\n",
    "def get10maxidx(l):\n",
    "    return list(map(lambda x: x[1], sorted(zip(l, range(0, len(l))), reverse=True)[:10]))\n",
    "def getmaxidx(l):\n",
    "    return l.index(max(l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "endangered-substance",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 10MWL classification\n",
    "clf = RandomForestClassifier(n_estimators=70)\n",
    "start_time2 = time.time()\n",
    "accuracies2_allf = []\n",
    "f1_scores2_macro_allf = []\n",
    "f1_scores2_weigh_allf = []\n",
    "for res in [obj[i] for i in l]:\n",
    "    # Input \n",
    "    X_data = list(map(lambda x: get10maxidx(x), res[1]))\n",
    "\n",
    "    # Output communities defined by Louvain algorithm\n",
    "    y_data = [comms_dict.get(str(i)) for i in range(1, len(comms_dict) + 1)]\n",
    "\n",
    "    # Split the data into training set and test set\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_data, y_data, test_size=0.2)\n",
    "\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = clf.predict(X_test)\n",
    "\n",
    "    ### Accuracy metric\n",
    "    accuracies2_allf.append(metrics.accuracy_score(y_test, y_pred))\n",
    "    f1_scores2_macro_allf.append(metrics.f1_score(y_test, y_pred, average=\"macro\"))\n",
    "    f1_scores2_weigh_allf.append(metrics.f1_score(y_test, y_pred, average=\"weighted\"))\n",
    "    \n",
    "end_time2 = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "commercial-prize",
   "metadata": {},
   "outputs": [],
   "source": [
    "### MWL classification\n",
    "clf = RandomForestClassifier(n_estimators=70)\n",
    "start_time3 = time.time()\n",
    "accuracies3_allf = []\n",
    "f1_scores3_macro_allf = []\n",
    "f1_scores3_weigh_allf = []\n",
    "for res in [obj[i] for i in l]:\n",
    "    # Input \n",
    "    X_data = list(map(lambda x: [getmaxidx(x)], res[1]))\n",
    "\n",
    "    # Output communities defined by Louvain algorithm\n",
    "    y_data = [comms_dict.get(str(i)) for i in range(1, len(comms_dict) + 1)]\n",
    "\n",
    "    # Split the data into training set and test set\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_data, y_data, test_size=0.2)\n",
    "\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = clf.predict(X_test)\n",
    "\n",
    "    ### Accuracy metric\n",
    "    accuracies3_allf.append(metrics.accuracy_score(y_test, y_pred))\n",
    "    f1_scores3_macro_allf.append(metrics.f1_score(y_test, y_pred, average=\"macro\"))\n",
    "    f1_scores3_weigh_allf.append(metrics.f1_score(y_test, y_pred, average=\"weighted\"))\n",
    "    \n",
    "end_time3 = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "affecting-hartford",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "matplotlib.use(\"pgf\")\n",
    "matplotlib.rcParams.update({\n",
    "    \"pgf.texsystem\": \"pdflatex\",\n",
    "    'font.family': 'serif',\n",
    "    'text.usetex': True,\n",
    "    'pgf.rcfonts': False\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "after-reducing",
   "metadata": {},
   "source": [
    "### Comparison macro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "theoretical-advance",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_16030/3460026992.py:18: UserWarning: Matplotlib is currently using pgf, which is a non-GUI backend, so cannot show the figure.\n",
      "  plt.show()\n"
     ]
    }
   ],
   "source": [
    "# Plot F1-macro comparison\n",
    "plt.figure(figsize=(10, 6))\n",
    "l = list(range(0,10)) + list(range(10, 30, 2))\n",
    "plt.plot(l, f1_scores_macro_allf, \"o\", label=\"AVPRA F1-score-macro\", markersize=10)\n",
    "plt.plot(l, f1_scores2_macro_allf, \"o\", label=\"AVPRA 10MWL F1-score-macro\", markersize=10)\n",
    "plt.plot(l, f1_scores3_macro_allf, \"o\", label=\"AVPRA MWL F1-score-macro\", markersize=10)\n",
    "\n",
    "plt.axvline(x=7, label=\"Diametro\", linestyle=\"--\")\n",
    "\n",
    "plt.xlabel(\"Iterazione\", fontsize=20)\n",
    "plt.ylabel(\"F1-score\", fontsize=20)\n",
    "plt.legend(loc=\"right\", prop={'size': 16})\n",
    "plt.xticks(fontsize=16)\n",
    "plt.yticks(fontsize=16)\n",
    "plt.ylim(0,1)\n",
    "\n",
    "plt.savefig(\"F1_twitch_allF_AVPRA_all_macro.png\", dpi=500)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "silver-aquatic",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8683384922391197, 3)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(f1_scores_macro_allf), f1_scores_macro_allf.index(max(f1_scores_macro_allf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "stopped-camping",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.7127304792607018, 6)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(f1_scores2_macro_allf), f1_scores2_macro_allf.index(max(f1_scores2_macro_allf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "deadly-fifteen",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.587931180907374, 0)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(f1_scores3_macro_allf), f1_scores3_macro_allf.index(max(f1_scores3_macro_allf))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "demonstrated-keeping",
   "metadata": {},
   "source": [
    "### Comparison weighted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "featured-madrid",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_16030/2850105035.py:18: UserWarning: Matplotlib is currently using pgf, which is a non-GUI backend, so cannot show the figure.\n",
      "  plt.show()\n"
     ]
    }
   ],
   "source": [
    "# Plot F1-macro comparison\n",
    "plt.figure(figsize=(10, 6))\n",
    "l = list(range(0,10)) + list(range(10, 30, 2))\n",
    "plt.plot(l, f1_scores_weigh_allf, \"o\", label=\"AVPRA F1-score-weighted\", markersize=10)\n",
    "plt.plot(l, f1_scores2_weigh_allf, \"o\", label=\"AVPRA 10MWL F1-score-weighted\", markersize=10)\n",
    "plt.plot(l, f1_scores3_weigh_allf, \"o\", label=\"AVPRA MWL F1-score-weighted\", markersize=10)\n",
    "\n",
    "plt.axvline(x=7, label=\"Diametro\", linestyle=\"--\")\n",
    "\n",
    "plt.xlabel(\"Iterazione\", fontsize=20)\n",
    "plt.ylabel(\"F1-score\", fontsize=20)\n",
    "plt.legend(loc=\"right\", prop={'size': 16})\n",
    "plt.xticks(fontsize=16)\n",
    "plt.yticks(fontsize=16)\n",
    "plt.ylim(0,1)\n",
    "\n",
    "plt.savefig(\"F1_twitch_allF_AVPRA_all_weighted.png\", dpi=500)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "sticky-payroll",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8514162046363131, 7)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(f1_scores_weigh_allf), f1_scores_weigh_allf.index(max(f1_scores_weigh_allf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "incorporate-institute",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.6702277740066446, 3)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(f1_scores2_weigh_allf), f1_scores2_weigh_allf.index(max(f1_scores2_weigh_allf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "split-baghdad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.3578809312879908, 0)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(f1_scores3_weigh_allf), f1_scores3_weigh_allf.index(max(f1_scores3_weigh_allf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "killing-fruit",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_16030/467218985.py:17: UserWarning: Matplotlib is currently using pgf, which is a non-GUI backend, so cannot show the figure.\n",
      "  plt.show()\n"
     ]
    }
   ],
   "source": [
    "# Plot accuracy graph\n",
    "plt.figure(figsize=(10, 6))\n",
    "l = list(range(0,10)) + list(range(10, 30, 2))\n",
    "plt.plot(l, accuracies_allf, \"o\", label=\"AVPRA Accuratezza\", markersize=10)\n",
    "plt.plot(l, f1_scores_macro_allf, \"x\", label=\"AVPRA F1-score-macro\", color=\"blue\", markersize=12)\n",
    "\n",
    "plt.axvline(x=7, label=\"Diametro\", linestyle=\"--\")\n",
    "\n",
    "plt.xlabel(\"Iterazione\", fontsize=20)\n",
    "plt.ylabel(\"Accuratezza/F1-Score\", fontsize=20)\n",
    "plt.legend(loc=\"right\", prop={'size': 16})\n",
    "plt.xticks(fontsize=16)\n",
    "plt.yticks(fontsize=16)\n",
    "plt.ylim(0,1)\n",
    "\n",
    "plt.savefig(\"F1_twitch_allF_AVPRA_macro.png\", dpi=500)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "economic-porcelain",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_16030/1320933098.py:17: UserWarning: Matplotlib is currently using pgf, which is a non-GUI backend, so cannot show the figure.\n",
      "  plt.show()\n"
     ]
    }
   ],
   "source": [
    "# Plot accuracy graph\n",
    "plt.figure(figsize=(10, 6))\n",
    "l = list(range(0,10)) + list(range(10, 30, 2))\n",
    "plt.plot(l, accuracies_allf, \"o\", label=\"AVPRA Accuratezza\", markersize=10)\n",
    "plt.plot(l, f1_scores_weigh_allf, \"x\", label=\"AVPRA F1-score-weighted\", color=\"blue\", markersize=12)\n",
    "\n",
    "plt.axvline(x=7, label=\"Diametro\", linestyle=\"--\")\n",
    "\n",
    "plt.xlabel(\"Iterazione\", fontsize=20)\n",
    "plt.ylabel(\"Accuratezza/F1-Score\", fontsize=20)\n",
    "plt.legend(loc=\"right\", prop={'size': 16})\n",
    "plt.xticks(fontsize=16)\n",
    "plt.yticks(fontsize=16)\n",
    "plt.ylim(0,1)\n",
    "\n",
    "plt.savefig(\"F1_twitch_allF_AVPRA_weighted.png\", dpi=500)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "current-accent",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8551289296017607, 7)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(accuracies_allf), l[accuracies_allf.index(max(accuracies_allf))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "beginning-combination",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.6921749992564613, 3)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(accuracies2_allf), l[accuracies2_allf.index(max(accuracies2_allf))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "toxic-thumbnail",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.4895755881390715, 0)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(accuracies3_allf), l[accuracies3_allf.index(max(accuracies3_allf))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "designed-convertible",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_16030/1469126784.py:18: UserWarning: Matplotlib is currently using pgf, which is a non-GUI backend, so cannot show the figure.\n",
      "  plt.show()\n"
     ]
    }
   ],
   "source": [
    "# Plot accuracy graph\n",
    "plt.figure(figsize=(10, 6))\n",
    "l = list(range(0,10)) + list(range(10, 30, 2))\n",
    "plt.plot(l, accuracies_allf, \"o\", label=\"AVPRA Accuratezza\", markersize=10)\n",
    "plt.plot(l, accuracies2_allf, \"o\", label=\"AVPRA 10MWL Accuratezza\", markersize=10)\n",
    "plt.plot(l, accuracies3_allf, \"o\", label=\"AVPRA MWL Accuratezza\", markersize=10)\n",
    "\n",
    "plt.axvline(x=7, label=\"Diametro\", linestyle=\"--\")\n",
    "\n",
    "plt.xlabel(\"Iterazione\", fontsize=20)\n",
    "plt.ylabel(\"Accuratezza\", fontsize=20)\n",
    "plt.legend(loc=\"lower right\", prop={'size': 16})\n",
    "plt.xticks(fontsize=16)\n",
    "plt.yticks(fontsize=16)\n",
    "plt.ylim(0,1)\n",
    "\n",
    "plt.savefig(\"F1_comparisons_twitch_allF.png\", dpi=500)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wrong-guidance",
   "metadata": {},
   "source": [
    "### Only micro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "focal-liver",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_16030/1879623748.py:16: UserWarning: Matplotlib is currently using pgf, which is a non-GUI backend, so cannot show the figure.\n",
      "  plt.show()\n"
     ]
    }
   ],
   "source": [
    "# Plot accuracy graph\n",
    "plt.figure(figsize=(10, 6))\n",
    "l = list(range(0,10)) + list(range(10, 30, 2))\n",
    "plt.plot(l, accuracies_allf, \"o\", label=\"AVPRA Accuratezza\", markersize=10)\n",
    "\n",
    "plt.axvline(x=7, label=\"Diametro\", linestyle=\"--\")\n",
    "\n",
    "plt.xlabel(\"Iterazione\", fontsize=20)\n",
    "plt.ylabel(\"Accuratezza\", fontsize=20)\n",
    "plt.legend(loc=\"right\", prop={'size': 16})\n",
    "plt.xticks(fontsize=16)\n",
    "plt.yticks(fontsize=16)\n",
    "plt.ylim(0,1)\n",
    "\n",
    "plt.savefig(\"F1_twitch_allF_micro.png\", dpi=500)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "charitable-grounds",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_16030/3013839329.py:16: UserWarning: Matplotlib is currently using pgf, which is a non-GUI backend, so cannot show the figure.\n",
      "  plt.show()\n"
     ]
    }
   ],
   "source": [
    "# Plot accuracy graph\n",
    "plt.figure(figsize=(10, 6))\n",
    "l = list(range(0,10)) + list(range(10, 30, 2))\n",
    "plt.plot(l, [accuracies[i] for i in l], \"o\", label=\"AVPRA Accuratezza\", markersize=10)\n",
    "\n",
    "plt.axvline(x=7, label=\"Diametro\", linestyle=\"--\")\n",
    "\n",
    "plt.xlabel(\"Iterazione\", fontsize=20)\n",
    "plt.ylabel(\"Accuratezza\", fontsize=20)\n",
    "plt.legend(loc=\"right\", prop={'size': 16})\n",
    "plt.xticks(fontsize=16)\n",
    "plt.yticks(fontsize=16)\n",
    "plt.ylim(0,1)\n",
    "\n",
    "plt.savefig(\"F1_twitch_onlyL_micro.png\", dpi=500)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "smaller-economics",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_16030/2823812909.py:17: UserWarning: Matplotlib is currently using pgf, which is a non-GUI backend, so cannot show the figure.\n",
      "  plt.show()\n"
     ]
    }
   ],
   "source": [
    "# Plot accuracy graph\n",
    "plt.figure(figsize=(10, 6))\n",
    "l = list(range(0,10)) + list(range(10, 30, 2))\n",
    "plt.plot(l, [accuracies[i] for i in l], \"o\", label=\"AVPRA Accuratezza\", markersize=10)\n",
    "plt.plot(l, accuracies_allf, \"o\", label=\"AVPRA* Accuratezza\", markersize=10)\n",
    "\n",
    "plt.axvline(x=7, label=\"Diametro\", linestyle=\"--\")\n",
    "\n",
    "plt.xlabel(\"Iterazione\", fontsize=20)\n",
    "plt.ylabel(\"Accuratezza\", fontsize=20)\n",
    "plt.legend(loc=\"right\", prop={'size': 16})\n",
    "plt.xticks(fontsize=16)\n",
    "plt.yticks(fontsize=16)\n",
    "plt.ylim(0,1)\n",
    "\n",
    "plt.savefig(\"F1_twitch_AVPRA_+_STAR.png\", dpi=500)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "convertible-talent",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
