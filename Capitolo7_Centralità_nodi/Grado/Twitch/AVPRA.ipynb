{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "genetic-communist",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Import useful libraries\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "import networkx as nx\n",
    "import time\n",
    "import random\n",
    "import csv\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "distinct-preliminary",
   "metadata": {},
   "outputs": [],
   "source": [
    "G = nx.read_edgelist(\"./edges_norm.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "charged-excitement",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computed degrees in: 8.0108642578125e-05\n"
     ]
    }
   ],
   "source": [
    "i_time = time.time()\n",
    "bet = nx.degree(G)\n",
    "print(f\"Computed degrees in: {time.time() - i_time}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "authorized-means",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Degrees normalization\n",
    "bet_l = []\n",
    "max_v = max(dict(bet).values())\n",
    "min_v = min(dict(bet).values())\n",
    "for i in range(1, len(bet) + 1):\n",
    "    bet_norm = (bet[str(i)] - min_v) / (max_v - min_v) \n",
    "    bet_l.append(bet_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "miniature-dealer",
   "metadata": {},
   "outputs": [],
   "source": [
    "l = list(range(0, 10)) + list(range(10, 30, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alternate-minority",
   "metadata": {},
   "source": [
    "# Heterogeneous 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "inside-frequency",
   "metadata": {},
   "outputs": [],
   "source": [
    "node_labels = []\n",
    "for bet_v in bet_l:\n",
    "    start_val = 0.0001\n",
    "    i = 1\n",
    "    while(True):\n",
    "        if bet_v <= start_val:\n",
    "            node_labels.append(i)\n",
    "            break\n",
    "        else:\n",
    "            i += 1\n",
    "            start_val *= 1.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "worse-parking",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7     20942\n",
       "8     19965\n",
       "6     19176\n",
       "9     16972\n",
       "5     15650\n",
       "1     15565\n",
       "10    12813\n",
       "4     12199\n",
       "11     8830\n",
       "2      7157\n",
       "3      6692\n",
       "12     5503\n",
       "13     3085\n",
       "14     1678\n",
       "15      857\n",
       "16      452\n",
       "17      267\n",
       "18      139\n",
       "19       86\n",
       "20       48\n",
       "21       23\n",
       "22        7\n",
       "24        4\n",
       "23        4\n",
       "dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(node_labels).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecological-yahoo",
   "metadata": {},
   "source": [
    "### AVPRA all feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "therapeutic-eligibility",
   "metadata": {},
   "outputs": [],
   "source": [
    "obj = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "second-founder",
   "metadata": {},
   "outputs": [],
   "source": [
    "obj = pd.read_pickle(\"./All_feat/log_trial_0_LPStates.pickled\") + pd.read_pickle(\"./All_feat/log_trial_1_LPStates.pickled\")[1:] + \\\n",
    "    pd.read_pickle(\"./All_feat/log_trial_2_LPStates.pickled\")[1:] + pd.read_pickle(\"./All_feat/log_trial_3_LPStates.pickled\")[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "tamil-spiritual",
   "metadata": {},
   "outputs": [],
   "source": [
    "obj = [obj[i] for i in l]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "experimental-gateway",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed iteration in 3.295698881149292\n",
      "Completed iteration in 14.949729204177856\n",
      "Completed iteration in 68.4543788433075\n",
      "Completed iteration in 75.6669020652771\n",
      "Completed iteration in 71.10734844207764\n",
      "Completed iteration in 76.8880672454834\n",
      "Completed iteration in 71.19984030723572\n",
      "Completed iteration in 74.14402294158936\n",
      "Completed iteration in 70.19854021072388\n",
      "Completed iteration in 73.52725791931152\n",
      "Completed iteration in 70.44321036338806\n",
      "Completed iteration in 71.62367868423462\n",
      "Completed iteration in 73.54709696769714\n",
      "Completed iteration in 75.40009808540344\n",
      "Completed iteration in 82.35192799568176\n",
      "Completed iteration in 85.70594906806946\n",
      "Completed iteration in 89.42560267448425\n",
      "Completed iteration in 102.54922676086426\n",
      "Completed iteration in 182.83702325820923\n",
      "Completed iteration in 175.81162977218628\n"
     ]
    }
   ],
   "source": [
    "### Random forest classifier creation with 70 trees\n",
    "clf = RandomForestClassifier(n_estimators=70)\n",
    "\n",
    "f1_scores_all_2 = []\n",
    "for res in obj:\n",
    "    start_time = time.time()\n",
    "   \n",
    "    X_data = list(map(lambda x: sorted(x, reverse=True), res[1]))\n",
    "    y_data = node_labels\n",
    "\n",
    "    # Split the data into training set and test set\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_data, y_data, test_size=0.2)\n",
    "\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = clf.predict(X_test)\n",
    "\n",
    "    ### Accuracy metric\n",
    "    f1_scores_all_2.append(metrics.f1_score(y_test, y_pred, average=\"micro\"))\n",
    "    \n",
    "    print(f\"Completed iteration in {time.time() - start_time}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "through-permission",
   "metadata": {},
   "source": [
    "### AVPRA only lang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "thirty-might",
   "metadata": {},
   "outputs": [],
   "source": [
    "obj = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "determined-bryan",
   "metadata": {},
   "outputs": [],
   "source": [
    "obj = pd.read_pickle(\"./Only_lang/log_trial_0_LPStates.pickled\") + pd.read_pickle(\"./Only_lang/log_trial_1_LPStates.pickled\")[1:] + \\\n",
    "    pd.read_pickle(\"./Only_lang/log_trial_2_LPStates.pickled\")[1:] + pd.read_pickle(\"./Only_lang/log_trial_3_LPStates.pickled\")[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "collaborative-scratch",
   "metadata": {},
   "outputs": [],
   "source": [
    "obj = [obj[i] for i in l]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "potential-section",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed iteration in 2.7884480953216553\n",
      "Completed iteration in 7.378140687942505\n",
      "Completed iteration in 64.6985182762146\n",
      "Completed iteration in 66.50079536437988\n",
      "Completed iteration in 64.4956910610199\n",
      "Completed iteration in 91.67194628715515\n",
      "Completed iteration in 97.7742965221405\n",
      "Completed iteration in 91.77427816390991\n",
      "Completed iteration in 97.6442494392395\n",
      "Completed iteration in 88.96565294265747\n",
      "Completed iteration in 91.07397770881653\n",
      "Completed iteration in 91.5280511379242\n",
      "Completed iteration in 74.62460565567017\n",
      "Completed iteration in 63.65137267112732\n",
      "Completed iteration in 63.599340200424194\n",
      "Completed iteration in 63.8297221660614\n",
      "Completed iteration in 64.31491041183472\n",
      "Completed iteration in 64.24934840202332\n",
      "Completed iteration in 70.38204574584961\n",
      "Completed iteration in 72.73244547843933\n"
     ]
    }
   ],
   "source": [
    "### Random forest classifier creation with 70 trees\n",
    "clf = RandomForestClassifier(n_estimators=70)\n",
    "\n",
    "f1_scores_only_2 = []\n",
    "for res in obj:\n",
    "    start_time = time.time()\n",
    "    \n",
    "    X_data = list(map(lambda x: sorted(x, reverse=True), res[1]))\n",
    "    y_data = node_labels\n",
    "\n",
    "    # Split the data into training set and test set\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_data, y_data, test_size=0.2)\n",
    "\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = clf.predict(X_test)\n",
    "\n",
    "    ### Accuracy metric\n",
    "    f1_scores_only_2.append(metrics.f1_score(y_test, y_pred, average=\"micro\"))\n",
    "    \n",
    "    print(f\"Completed iteration in {time.time() - start_time}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "proof-genome",
   "metadata": {},
   "source": [
    "# Heterogeneous 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "threatened-islam",
   "metadata": {},
   "outputs": [],
   "source": [
    "node_labels = []\n",
    "for bet_v in bet_l:\n",
    "    ok = False\n",
    "    for i in range(1, 11):\n",
    "        if(bet_v <= 0.005 * i): \n",
    "            node_labels.append(i)\n",
    "            ok = True\n",
    "            break\n",
    "    if ok: continue\n",
    "    for i in range(1, 6):\n",
    "        if(bet_v <= 0.05 + 0.01 * i):\n",
    "            node_labels.append(10 + i)\n",
    "            ok = True\n",
    "            break\n",
    "    if ok: continue\n",
    "    if(bet_v <= 0.2):\n",
    "        node_labels.append(16)\n",
    "        continue\n",
    "    node_labels.append(17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "relative-saint",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1     153285\n",
       "2       9471\n",
       "3       2516\n",
       "4       1036\n",
       "5        530\n",
       "6        288\n",
       "7        207\n",
       "8        133\n",
       "16       127\n",
       "9        104\n",
       "11       104\n",
       "12        83\n",
       "10        75\n",
       "13        47\n",
       "17        45\n",
       "14        38\n",
       "15        25\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(node_labels).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "white-heritage",
   "metadata": {},
   "source": [
    "### AVPRA all feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "regional-midnight",
   "metadata": {},
   "outputs": [],
   "source": [
    "obj = pd.read_pickle(\"./All_feat/log_trial_0_LPStates.pickled\") + pd.read_pickle(\"./All_feat/log_trial_1_LPStates.pickled\")[1:] + \\\n",
    "    pd.read_pickle(\"./All_feat/log_trial_2_LPStates.pickled\")[1:] + pd.read_pickle(\"./All_feat/log_trial_3_LPStates.pickled\")[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "indian-forth",
   "metadata": {},
   "outputs": [],
   "source": [
    "obj = [obj[i] for i in l]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "industrial-saskatchewan",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed iteration in 2.9221246242523193\n",
      "Completed iteration in 10.6644606590271\n",
      "Completed iteration in 56.74378800392151\n",
      "Completed iteration in 52.653939723968506\n",
      "Completed iteration in 57.65579438209534\n",
      "Completed iteration in 53.43042469024658\n",
      "Completed iteration in 56.43837070465088\n",
      "Completed iteration in 50.85349988937378\n",
      "Completed iteration in 60.2674765586853\n",
      "Completed iteration in 57.20895314216614\n",
      "Completed iteration in 59.12400770187378\n",
      "Completed iteration in 56.3473482131958\n",
      "Completed iteration in 60.291446685791016\n",
      "Completed iteration in 59.01368713378906\n",
      "Completed iteration in 58.61095714569092\n",
      "Completed iteration in 63.479398250579834\n",
      "Completed iteration in 66.34580540657043\n",
      "Completed iteration in 85.44818353652954\n",
      "Completed iteration in 177.91800451278687\n",
      "Completed iteration in 177.24749970436096\n"
     ]
    }
   ],
   "source": [
    "### Random forest classifier creation with 70 trees\n",
    "clf = RandomForestClassifier(n_estimators=70)\n",
    "\n",
    "f1_scores_all_1 = []\n",
    "for res in obj:\n",
    "    start_time = time.time()\n",
    "   \n",
    "    X_data = list(map(lambda x: sorted(x, reverse=True), res[1]))\n",
    "    y_data = node_labels\n",
    "\n",
    "    # Split the data into training set and test set\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_data, y_data, test_size=0.2)\n",
    "\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = clf.predict(X_test)\n",
    "\n",
    "    ### Accuracy metric\n",
    "    f1_scores_all_1.append(metrics.f1_score(y_test, y_pred, average=\"micro\"))\n",
    "    \n",
    "    print(f\"Completed iteration in {time.time() - start_time}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "selective-flash",
   "metadata": {},
   "source": [
    "### AVPRA only lang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "adopted-bridal",
   "metadata": {},
   "outputs": [],
   "source": [
    "obj = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "running-aircraft",
   "metadata": {},
   "outputs": [],
   "source": [
    "obj = pd.read_pickle(\"./Only_lang/log_trial_0_LPStates.pickled\") + pd.read_pickle(\"./Only_lang/log_trial_1_LPStates.pickled\")[1:] + \\\n",
    "    pd.read_pickle(\"./Only_lang/log_trial_2_LPStates.pickled\")[1:] + pd.read_pickle(\"./Only_lang/log_trial_3_LPStates.pickled\")[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "vocational-metro",
   "metadata": {},
   "outputs": [],
   "source": [
    "obj = [obj[i] for i in l]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "superb-handy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed iteration in 2.346672773361206\n",
      "Completed iteration in 5.972807884216309\n",
      "Completed iteration in 44.83026480674744\n",
      "Completed iteration in 42.62898278236389\n",
      "Completed iteration in 46.28037476539612\n",
      "Completed iteration in 43.704256534576416\n",
      "Completed iteration in 47.120314598083496\n",
      "Completed iteration in 44.27061653137207\n",
      "Completed iteration in 46.55402874946594\n",
      "Completed iteration in 44.44453239440918\n",
      "Completed iteration in 45.05700993537903\n",
      "Completed iteration in 45.38632321357727\n",
      "Completed iteration in 46.87142825126648\n",
      "Completed iteration in 45.50168442726135\n",
      "Completed iteration in 45.62984037399292\n",
      "Completed iteration in 47.036736249923706\n",
      "Completed iteration in 46.750805139541626\n",
      "Completed iteration in 46.6671416759491\n",
      "Completed iteration in 50.35727334022522\n",
      "Completed iteration in 55.68504071235657\n"
     ]
    }
   ],
   "source": [
    "### Random forest classifier creation with 70 trees\n",
    "clf = RandomForestClassifier(n_estimators=70)\n",
    "\n",
    "f1_scores_only_1 = []\n",
    "for res in obj:\n",
    "    start_time = time.time()\n",
    "    \n",
    "    X_data = list(map(lambda x: sorted(x, reverse=True), res[1]))\n",
    "    y_data = node_labels\n",
    "\n",
    "    # Split the data into training set and test set\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_data, y_data, test_size=0.2)\n",
    "\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = clf.predict(X_test)\n",
    "\n",
    "    ### Accuracy metric\n",
    "    f1_scores_only_1.append(metrics.f1_score(y_test, y_pred, average=\"micro\"))\n",
    "    \n",
    "    print(f\"Completed iteration in {time.time() - start_time}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "textile-counter",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "executive-saying",
   "metadata": {},
   "source": [
    "### Only lang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "dirty-ghana",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9915831424917467, 1)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Intervals b\n",
    "max(f1_scores_only_1), l[f1_scores_only_1.index(max(f1_scores_only_1))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "southern-toolbox",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.6125866222526247, 1)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Intervals a\n",
    "max(f1_scores_only_2), l[f1_scores_only_2.index(max(f1_scores_only_2))]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "limiting-geneva",
   "metadata": {},
   "source": [
    "### All features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "geological-centre",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9916128840377123, 1)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Intervals b\n",
    "max(f1_scores_all_1), l[f1_scores_all_1.index(max(f1_scores_all_1))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "negative-deployment",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8716057460666805, 1)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Intervals a\n",
    "max(f1_scores_all_2), l[f1_scores_all_2.index(max(f1_scores_all_2))]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "behind-thong",
   "metadata": {},
   "source": [
    "### Graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "collective-finding",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "matplotlib.use(\"pgf\")\n",
    "matplotlib.rcParams.update({\n",
    "    \"pgf.texsystem\": \"pdflatex\",\n",
    "    'font.family': 'serif',\n",
    "    'text.usetex': True,\n",
    "    'pgf.rcfonts': False\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "religious-trinidad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_8460/3465300998.py:14: UserWarning: Matplotlib is currently using pgf, which is a non-GUI backend, so cannot show the figure.\n",
      "  plt.show()\n"
     ]
    }
   ],
   "source": [
    "### Degree distrib function\n",
    "serie = pd.Series(bet_l)\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(serie, 200)\n",
    "plt.xlabel(\"Grado\", fontsize=22)\n",
    "plt.ylabel(\"Frequenza assoluta\", fontsize=22)\n",
    "plt.xticks(fontsize=16)\n",
    "plt.yticks(fontsize=16)\n",
    "\n",
    "plt.xlim(-0.01, 1)\n",
    "\n",
    "plt.savefig(\"Distribution_norm_Degree.png\", dpi=500)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "spanish-miller",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_23699/4041381733.py:18: UserWarning: Matplotlib is currently using pgf, which is a non-GUI backend, so cannot show the figure.\n",
      "  plt.show()\n"
     ]
    }
   ],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "l = list(range(0, 10)) + list(range(10, 30, 2))\n",
    "\n",
    "plt.plot(l, f1_scores_all_2, \"o\", label=\"F1-score intervalli [a]\", markersize=10)\n",
    "plt.plot(l, f1_scores_all_1, \"o\", label=\"F1-score intervalli [b]\", markersize=10)\n",
    "\n",
    "plt.xlabel(\"Iterazione\", fontsize=22)\n",
    "plt.ylabel(\"F1-score\", fontsize=22)\n",
    "\n",
    "plt.xticks(fontsize=16)\n",
    "plt.yticks(fontsize=16)\n",
    "\n",
    "plt.ylim(0, 1)\n",
    "\n",
    "plt.legend(loc=\"right\", prop={'size': 16})\n",
    "\n",
    "plt.savefig(\"Micro_comparison_allF_Degree.png\", dpi=500)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "progressive-channel",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_23699/1432725026.py:18: UserWarning: Matplotlib is currently using pgf, which is a non-GUI backend, so cannot show the figure.\n",
      "  plt.show()\n"
     ]
    }
   ],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "l = list(range(0, 10)) + list(range(10, 30, 2))\n",
    "\n",
    "plt.plot(l, f1_scores_only_2, \"o\", label=\"F1-score intervalli [a]\", markersize=10)\n",
    "plt.plot(l, f1_scores_only_1, \"o\", label=\"F1-score intervalli [b]\", markersize=10)\n",
    "\n",
    "plt.xlabel(\"Iterazione\", fontsize=22)\n",
    "plt.ylabel(\"F1-score\", fontsize=22)\n",
    "\n",
    "plt.xticks(fontsize=16)\n",
    "plt.yticks(fontsize=16)\n",
    "\n",
    "plt.ylim(0, 1)\n",
    "\n",
    "plt.legend(loc=\"right\", prop={'size': 16})\n",
    "\n",
    "plt.savefig(\"Micro_comparison_onlyL_Degree.png\", dpi=500)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "floating-scope",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_23699/2096777536.py:22: UserWarning: Matplotlib is currently using pgf, which is a non-GUI backend, so cannot show the figure.\n",
      "  plt.show()\n"
     ]
    }
   ],
   "source": [
    "### With o and x\n",
    "plt.figure(figsize=(10, 6))\n",
    "l = list(range(0, 10)) + list(range(10, 30, 2))\n",
    "\n",
    "plt.plot(l, f1_scores_only_2, \"o\", label=\"AVPRA F1-score intervalli [a]\", markersize=10)\n",
    "plt.plot(l, f1_scores_only_1, \"o\", label=\"AVPRA F1-score intervalli [b]\", markersize=10)\n",
    "\n",
    "plt.plot(l, f1_scores_all_2, \"x\", label=\"AVPRA* F1-score intervalli [a]\", color=\"blue\", markersize=12)\n",
    "plt.plot(l, f1_scores_all_1, \"x\", label=\"AVPRA* F1-score intervalli [b]\", color=\"red\", markersize=12)\n",
    "\n",
    "plt.xlabel(\"Iterazione\", fontsize=22)\n",
    "plt.ylabel(\"F1-score\", fontsize=22)\n",
    "\n",
    "plt.xticks(fontsize=16)\n",
    "plt.yticks(fontsize=16)\n",
    "\n",
    "plt.ylim(0, 1)\n",
    "\n",
    "plt.legend(loc=\"right\", prop={'size': 16})\n",
    "\n",
    "plt.savefig(\"Micro_comparison_both_Degree.png\", dpi=500)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "eight-paris",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_23699/2383931694.py:22: UserWarning: Matplotlib is currently using pgf, which is a non-GUI backend, so cannot show the figure.\n",
      "  plt.show()\n"
     ]
    }
   ],
   "source": [
    "### With o\n",
    "plt.figure(figsize=(10, 6))\n",
    "l = list(range(0, 10)) + list(range(10, 30, 2))\n",
    "\n",
    "plt.plot(l, f1_scores_only_2, \"o\", label=\"AVPRA F1-score intervalli [a]\", markersize=10)\n",
    "plt.plot(l, f1_scores_only_1, \"o\", label=\"AVPRA F1-score intervalli [b]\", markersize=10)\n",
    "\n",
    "plt.plot(l, f1_scores_all_2, \"o\", label=\"AVPRA* F1-score intervalli [a]\", markersize=10)\n",
    "plt.plot(l, f1_scores_all_1, \"o\", label=\"AVPRA* F1-score intervalli [b]\", markersize=10)\n",
    "\n",
    "plt.xlabel(\"Iterazione\", fontsize=22)\n",
    "plt.ylabel(\"F1-score\", fontsize=22)\n",
    "\n",
    "plt.xticks(fontsize=16)\n",
    "plt.yticks(fontsize=16)\n",
    "\n",
    "plt.ylim(0, 1)\n",
    "\n",
    "plt.legend(loc=\"right\", prop={'size': 16})\n",
    "\n",
    "plt.savefig(\"Micro_comparison_both_O_Degree.png\", dpi=500)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "static-knowing",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
